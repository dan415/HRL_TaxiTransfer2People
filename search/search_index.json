{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Hierarchical Reinforcement Learning Method for OpenAI Gym's Taxi-v3 Environment. Introduction This project has been developed as the final project for the Reinforcement Learning course in the Master's in Artificial Intelligence at Universidad Carlos III de Madrid. The research explores the performance of Hierarchical Reinforcement Learning (HRL) in comparison to the traditional approach of reinforcement learning, specifically in the OpenAI Gym Taxi environment (Brockman et al. 2016). In this case, it is suggested that applying a hierarchical structure in the learning process can significantly improve the agent's effectiveness and efficiency in solving tasks. Through systematic experiments and comparative evaluations, we demonstrate that the HRL approach achieves superior performance in terms of convergence time and final performance compared to traditional reinforcement learning methods, primarily for complex environments such as those involving multiple passengers. These results support the efficiency of the hierarchical approach in enhancing the agent's adaptability and decision-making capabilities in the specific context of the taxi problem. This work contributes to understanding the applicability and benefits of hierarchical reinforcement learning in practical environments. By transferring the learned hierarchy from a single-person problem to an extended version involving two people, we achieve a more robust learning of the task in fewer episodes compared to a strategy solely based on Q-Learning. The hierarchy extension involves treating GET and PUT tasks as parameterized tasks (p), analogous to what the original paper proposes for the Navigate task, where p represents the person on whom to perform the operation. Installation Python version used: 3.10 Using conda: conda create -f environment.yml cd src pip install -e custom Using pip: pip install -r requirements.txt cd src pip install -e custom Author Daniel Cabrera Rodr\u00edguez Github: @dan415 License TODO","title":"Home"},{"location":"custom/rationale.html","text":"Taxi2PEnv Taxi2PEnv is an extension of the classic Taxi-v3 environment. This modified environment supports two passengers, each with its own destination. The environment has 6 discrete deterministic actions: 0: move south 1: move north 2: move east 3: move west 4: pickup passenger 5: drop off passenger Observation Space There are 10.000 discrete states based on the taxi's position, passenger locations, and destinations. The state space is represented by the tuple (taxi_row, taxi_col, pass_loc1, pass_loc2, dest_idx1, dest_idx2). The original Taxi-v3 environment has a state space of 500 discrete states, represented by the tuple (taxi_row, taxi_col, pass_loc, dest_idx). Adding a new passenger implies adding a new destination to the environment, and a new passenger location. The passenger location can take any of the 5 values (R, G, Y, B, and in taxi), and the destination can take any of the 4 values (R, G, Y, B). Therefore, we would be incrementing the state space by 20 (5 * 4). Action Space There are 6 discrete deterministic actions, as described above. Rewards: -1 per step unless other rewards are triggered. +20 for delivering both passengers. -10 for executing \"pickup\" and \"drop-off\" actions illegally. It is considered illegal to pick up a passenger that has already been dropped off at their destination. +10 for delivering the first passenger. There is no restriction to the initialization of states, except for states where any of the passengers spawn inside the taxi or at their destination. Translation of States The translation of states between hierarchical levels is crucial during both state transitions and policy evaluation to maintain state integrity. The translation process is implemented as follows: Translation of states from the high dimensional space is done by selecting what passenger to translate the state for. Once the passenger is selected, the state is translated in the same way as Taxi-V3 environment. Encoding functions, as usually, need to be defined by all the state space attributes. These methods are provided as utils in the package: encode_taxi1P Encodes the state of the environment for a single passenger. Parameters: - taxi_row (int) \u2013 Row of the taxi - taxi_col (int) \u2013 Column of the taxi - pass_loc (int) \u2013 Location of the passenger - dest_idx (int) \u2013 Index of the destination Returns: - state (int) \u2013 Encoded state decode_taxi1P Decodes the state of the environment for a single passenger. Parameters: - state (int) \u2013 Encoded state Returns: Tuple of: - taxi_row (int) \u2013 Row of the taxi - taxi_col (int) \u2013 Column of the taxi - pass_loc (int) \u2013 Location of the passenger - dest_idx (int) \u2013 Index of the destination encode_taxi2P Encodes the state of the environment for two passengers. Parameters: - taxi_row (int) \u2013 Row of the taxi - taxi_col (int) \u2013 Column of the taxi - pass_loc1 (int) \u2013 Location of the first passenger - pass_loc2 (int) \u2013 Location of the second passenger - dest_idx1 (int) \u2013 Index of the first destination - dest_idx2 (int) \u2013 Index of the second destination Returns: - state (int) \u2013 Encoded state decode_taxi2P Decodes the state of the environment for two passengers. Parameters: - state (int) \u2013 Encoded state Returns: Tuple of: - taxi_row (int) \u2013 Row of the taxi - taxi_col (int) \u2013 Column of the taxi - pass_loc1 (int) \u2013 Location of the first passenger - pass_loc2 (int) \u2013 Location of the second passenger - dest_idx1 (int) \u2013 Index of the first destination - dest_idx2 (int) \u2013 Index of the second destination translate Translates the state of the environment from the high dimensional space to the low dimensional space. Parameters: - state (int) \u2013 Encoded state - passenger (int) \u2013 Passenger to translate the state for (it can be 1 or 2) Returns: - state (int) \u2013 Translated encoded state in low dimensional space Note: The maximum number of steps allowed for this environment is set to 1000.","title":"2 People Taxi Environment"},{"location":"custom/rationale.html#encode_taxi1p","text":"Encodes the state of the environment for a single passenger. Parameters: - taxi_row (int) \u2013 Row of the taxi - taxi_col (int) \u2013 Column of the taxi - pass_loc (int) \u2013 Location of the passenger - dest_idx (int) \u2013 Index of the destination Returns: - state (int) \u2013 Encoded state","title":"encode_taxi1P"},{"location":"custom/rationale.html#decode_taxi1p","text":"Decodes the state of the environment for a single passenger. Parameters: - state (int) \u2013 Encoded state Returns: Tuple of: - taxi_row (int) \u2013 Row of the taxi - taxi_col (int) \u2013 Column of the taxi - pass_loc (int) \u2013 Location of the passenger - dest_idx (int) \u2013 Index of the destination","title":"decode_taxi1P"},{"location":"custom/rationale.html#encode_taxi2p","text":"Encodes the state of the environment for two passengers. Parameters: - taxi_row (int) \u2013 Row of the taxi - taxi_col (int) \u2013 Column of the taxi - pass_loc1 (int) \u2013 Location of the first passenger - pass_loc2 (int) \u2013 Location of the second passenger - dest_idx1 (int) \u2013 Index of the first destination - dest_idx2 (int) \u2013 Index of the second destination Returns: - state (int) \u2013 Encoded state","title":"encode_taxi2P"},{"location":"custom/rationale.html#decode_taxi2p","text":"Decodes the state of the environment for two passengers. Parameters: - state (int) \u2013 Encoded state Returns: Tuple of: - taxi_row (int) \u2013 Row of the taxi - taxi_col (int) \u2013 Column of the taxi - pass_loc1 (int) \u2013 Location of the first passenger - pass_loc2 (int) \u2013 Location of the second passenger - dest_idx1 (int) \u2013 Index of the first destination - dest_idx2 (int) \u2013 Index of the second destination","title":"decode_taxi2P"},{"location":"custom/rationale.html#translate","text":"Translates the state of the environment from the high dimensional space to the low dimensional space. Parameters: - state (int) \u2013 Encoded state - passenger (int) \u2013 Passenger to translate the state for (it can be 1 or 2) Returns: - state (int) \u2013 Translated encoded state in low dimensional space Note: The maximum number of steps allowed for this environment is set to 1000.","title":"translate"},{"location":"experiments/hrl/1person/rationale.html","text":"Hierarchical Agent for Two Persons The experiment implemented in this project is based on the Q-Learning algorithm, serving as a performance benchmark for Hierarchical Reinforcement Learning (HRL) agents. The HRL agents are designed to operate in an environment modeled after the Taxi problem, with a focus on hierarchical structures and task representation. The core Markov Decision Process (MDP) is denoted as \"Root,\" overseeing the hierarchy. The algorithm used is MaxQ-0, which is a hierarchical extension of Q-Learning, with an Epsilon Greeedy policy without decay. Task Representation The experiment explores the representation of the \"Navigate\" task, a key component of the HRL framework. Previous works have proposed different parameterizations for the task, including representing the destination or passenger location. The implementation considers two options, with a primary emphasis on the representation where \"f\" corresponds to the place to navigate. Complexity Considerations The choice of task representation is influenced by a desire to minimize the cardinality of possible parameters. This is crucial, given that the \"Navigate\" task must be learned for each parameter, and increasing cardinality implies increased complexity. While an alternative representation is explored for its versatility, the primary focus is on the first option due to its potential for reduced training episodes. Policy Implementation Across all experiments, an Epsilon Greedy policy with a fixed epsilon value is employed during training. This policy guides the agent's exploration-exploitation trade-off throughout the learning process. Termination Conditions The termination conditions for tasks vary, with the \"Root\" task concluding when the episode is either successfully resolved or reaches the maximum number of iterations. The \"Get\" and \"Put\" tasks terminate based on the passenger's presence in the taxi. For the \"Navigate\" task, termination occurs when the taxi's coordinates match the target, and the passenger is either in or out of the taxi, depending on the task variant. Command Line Arguments --experiment (type: str, default: None): Specifies the name of the experiment. If the experiment name doesn't start with \"hrl_transfer2nav\" or is not provided, the script assumes that training is required. --episodes (type: int, default: 10000): Sets the number of episodes for training or testing the agent. --steps (type: int, default: 300): Defines the maximum number of steps per episode. --alpha (type: float, default: 0.2): Sets the learning rate (alpha) for the agent. It determines how much the agent updates its Q-values based on new experiences. --gamma (type: float, default: 1): Sets the discount factor (gamma), which influences the importance of future rewards in the agent's decision-making. --epsilon (type: float, default: 0.004): Sets the exploration-exploitation factor (epsilon). It controls the likelihood of the agent choosing a random action instead of exploiting its current knowledge. --show_plot (action: store_true): When provided, this flag indicates that a plot of training and test rewards should be displayed after the experiment. --render_training (action: store_true): When provided, this flag indicates that the environment should be rendered during training.","title":"Rationale"},{"location":"experiments/hrl/2people/rationale.html","text":"Hierarchical Agent for Two Persons For the hierarchical agent designed to handle two persons, the transfer learning process involves freezing policies for all tasks except \"Root.\" This enables the agent to learn a policy that determines which action and person choice minimize execution time. The state translation process is integral during state transitions and recursive policy evaluation to ensure the preservation of state integrity. Transfer Learning The project incorporates transfer learning, starting with training an agent for the Taxi V3.0 environment. To facilitate transfer, knowledge from the \"Get\" and \"Put\" tasks is duplicated in the corresponding tables, while the \"Root\" task's content is overwritten or removed. The transfer involves transitioning from 500 to 10,000 states, necessitating a freeze on policies for all tasks except \"Root.\" The translation of states between hierarchical levels is crucial during both state transitions and policy evaluation to maintain state integrity. We first need to train an agent for the Taxi V3.0 environment. In order to achieve this, we provide the maxq experiment implementations. Please note that, as we provide two different options with designs of the Navigation Task, the appropiate version of the experiment should be used for its corresponding extension for two persons. Then, we will have on a experiments folder a subfolder generated with the name of the experiment (it's based on execution datetime). Inside this folder, we will have the following files: * config.json : Contains the configuration of the experiment. It is used for tracking the experiment parameters. It saves the following information: json { \"experiment_name\": \"hrl_transfer2nav_2p\", \"episodes\": 10000, \"steps\": 300, \"alpha\": 0.2, \"gamma\": 1, \"epsilon\": 0.004, \"show_plot\": true, \"render_training\": false } plot.png : plot of training and test rewards. See Utils section for more information. plot.html : HTML version of the plot ctable.npy : C-table (3 dimensional state-action-state (500 x n_actions x 500) matrix) for the experiment. Needed for testing the agent. n_actions can be either 13 or 15, depending on the Navigation Task design. vtable.npy : V-table (2 dmensional state-action (500 x n_actions) matrix) for the experiment. Needed for testing the agent. n_actions can be either 13 or 15, depending on the Navigation Task design. Then, we need to provide the experiment name to the transfer learning experiment. The experiment name is the name of the folder generated in the previous step. The agent, when passing it this low dimensional tables, will automatically expand them to the 10.000 states. This process involves copying the GET, PUT tasks and adding them as new task (the first two will be used for passenger 1, the second two for the passenger 2). The Root task will be overwritten, and will need to relearned. This here is the main purpose of the transfer. By doing this, we can have all the hierarchy learned except for the Root Task, and we will only need to train for it. The Root task will learn how and when to select a passenger and the GET or PUT action. This reduces training time significantly, as oppose to learning the whole hierarchy from scratch. When training the Root task, the rest of the hierarchy will be frozen, and will not be updated. The training will be saved in the same manner as the previous experiment, but with the new experiment name. The new experiment name will be the same as the previous one, but the states dimension will be 10.000. Command Line Arguments --experiment (type: str, default: None): Specifies the name of the experiment. If the experiment name is not provided, the script assumes that training is required, and will train from scrathc. If the dimension of the states in the provided tables is 500, it will assume that the experiment is for the Taxi V3.0 environment. If the dimension is 10.000, it will assume that the experiment needs transfer learning, and will create the tables as described in the previous section, and will train the Root task from scratch. If the dimension is 10.000 a it will assume that the experiment is for the Taxi2P environment, and will not train, but will test the agent, rendering the environment. --episodes (type: int, default: 10000): Sets the number of episodes for training or testing the agent. --steps (type: int, default: 300): Defines the maximum number of steps per episode. --alpha (type: float, default: 0.2): Sets the learning rate (alpha) for the agent. It determines how much the agent updates its Q-values based on new experiences. --gamma (type: float, default: 1): Sets the discount factor (gamma), which influences the importance of future rewards in the agent's decision-making. --epsilon (type: float, default: 0.004): Sets the exploration-exploitation factor (epsilon). It controls the likelihood of the agent choosing a random action instead of exploiting its current knowledge. --show_plot (action: store_true): When provided, this flag indicates that a plot of training and test rewards should be displayed after the experiment. --render_training (action: store_true): When provided, this flag indicates that the training process should be visually rendered, showing the agent's interaction with the environment.","title":"Rationale"},{"location":"experiments/qlearning/rationale.html","text":"Q-Learning Implementation> The Q-Learning algorithm is implemented in the qlearning module. Although I have specifically created a version for 1 person and 2 persons, it is, in essence, the same algorithm. This implementation is provided as a baseline for comparing the performance of the HRL agents. The policy implemented is decaying Epsilon Greedy, with a fixed epsilon value. The experiment folder is qlearning , and it contains the following files: config.json : Contains the configuration of the experiment. It is used for tracking the experiment parameters. It saves the following information: json { \"learning_rate\": 0.1, \"discount_rate\": 0.99, \"decay_rate\": 0.001, \"episodes\": 10000, \"steps\": 300, \"render_training\": false, \"show_plot\": true } plot.png : plot of training and test rewards. See Utils section for more information. plot.html : HTML version of the plot qtable.npy : Q-table (2 dmensional state-action (500 x n_actions) matrix) for the experiment. Needed for testing the agent. n_actions can be either 13 or 15, depending on the Navigation Task design. Command Line Arguments --qtable (type: str, default: None): Specifies the name of the Q-table to be used for training or testing the agent. If the Q-table is not provided, the script assumes that training is required. --episodes (type: int, default: 10000): Sets the number of episodes for training or testing the agent. --steps (type: int, default: 300): Defines the maximum number of steps per episode. --discount_rate (type: float, default: 0.99): Sets the discount rate (gamma), which influences the importance of future rewards in the agent's decision-making. --decay_rate (type: float, default: 0.001): Sets the decay rate for the epsilon value. It determines how much the epsilon value decreases after each episode, controlling exploration vs exploitation. --learning_rate (type: float, default: 0.1): Sets the learning rate (alpha) for the agent. It determines how much the agent updates its Q-values based on new experiences. --epsilon (type: float, default: 0.1): Sets the exploration-exploitation factor (epsilon). It controls the likelihood of the agent choosing a random action instead of exploiting its current knowledge. --show_plot (action: store_true): When provided, this flag indicates that a plot of training and test rewards should be displayed after the experiment.","title":"Rationale"}]}